#!/usr/bin/env python3
"""
Persona Agent - A flexible, prompt-driven evaluation agent for BB-project.
"""
import json
import os
import re
from typing import List, Dict, Any

# Assuming a library for LLM calls exists
# from some_api import call_llm 

class PersonaAgent:
    """
    A single, flexible persona agent that evaluates scripts based on a dynamic prompt.
    The persona, criteria, and scripts to evaluate are not hardcoded but provided
    in a comprehensive prompt at runtime.
    """
    
    def __init__(self, persona_id: str):
        self.persona_id = persona_id
        self.model = "claude-3-haiku-20240307" # Use a fast model for evaluation

    def _call_anthropic_api(self, prompt: str) -> str:
        """
        Placeholder for calling a powerful language model like Claude.
        This simulates the API call and returns a structured JSON string.
        """
        print(f"ğŸ¤– {self.persona_id}: LLM APIå‘¼ã³å‡ºã—ï¼ˆè©•ä¾¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰...")
        import time
        time.sleep(10) # Evaluation takes time

        # This is a mock response. A real LLM would generate this based on the prompt.
        mock_evaluations = {}
        # The prompt contains script contents, so we can't easily know the IDs here.
        # This mock will just create dummy data for 15 scripts.
        for i in range(1, 16):
            script_key = f"script_{i}" # A generic key
            total_score = 70 + (i % 5) * 5
            mock_evaluations[script_key] = {
                "total_score": total_score,
                "feedback": f"[{self.persona_id}ã¨ã—ã¦] ã“ã®å°æœ¬ã¯ãªã‹ãªèˆˆå‘³æ·±ã„ã§ã™ãŒã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹ã«... (ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯)",
                "breakdown": {
                    "é …ç›®1": {"score": total_score - 10, "feedback": "å…·ä½“çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯..."},
                    "é …ç›®2": {"score": total_score + 5, "feedback": "å…·ä½“çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯..."},
                }
            }
        
        mock_response = {"evaluations": mock_evaluations}
        
        print(f"ğŸ¤– {self.persona_id}: LLM APIå¿œç­”å—ä¿¡ï¼ˆè©•ä¾¡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        return json.dumps(mock_response)

    def evaluate_scripts(self, full_prompt: str) -> Dict[str, Any]:
        """
        Evaluates multiple scripts based on a single, comprehensive prompt.
        The prompt should contain the persona definition, evaluation criteria,
        and all scripts to be evaluated.
        """
        print(f"ğŸ“Š {self.persona_id}: æ–°ã—ã„äººæ ¼ã¨åŸºæº–ã«åŸºã¥ãã€å…¨å°æœ¬ã®ä¸€æ‹¬è©•ä¾¡ã‚’é–‹å§‹...")

        system_prompt = """
ã‚ãªãŸã¯ã€ä¸ãˆã‚‰ã‚ŒãŸã€Œãƒšãƒ«ã‚½ãƒŠï¼ˆäººæ ¼ï¼‰ã€ã«å®Œç’§ã«ãªã‚Šãã‚Šã€æŒ‡å®šã•ã‚ŒãŸã€Œè©•ä¾¡åŸºæº–ã€ã ã‘ã‚’ä½¿ã£ã¦ã€æç¤ºã•ã‚ŒãŸè¤‡æ•°ã®ã€Œè©•ä¾¡å¯¾è±¡å°æœ¬ã€ã‚’å³å¯†ã«è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
è©•ä¾¡çµæœã¯ã€å¿…ãšæŒ‡ç¤ºã•ã‚ŒãŸJSONå½¢å¼ã§ã€ã‚­ãƒ¼ã¨å€¤ã‚’æ­£ç¢ºã«å®ˆã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã€è§£èª¬ã€è¨€ã„è¨³ãªã©ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""

        final_user_prompt = f"""
{full_prompt}

---
ã€æœ€çµ‚ã‚¿ã‚¹ã‚¯ã€‘
ä¸Šè¨˜ã®ã€Œã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠã€ã€Œè©•ä¾¡åŸºæº–ã€ã€Œè©•ä¾¡å¯¾è±¡å°æœ¬ã€ã®3ã¤ã®æƒ…å ±ã‚’å®Œå…¨ã‹ã¤å³å¯†ã«å®ˆã‚Šã€ã™ã¹ã¦ã®å°æœ¬ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

1.  **ãƒšãƒ«ã‚½ãƒŠã¸ã®æ²¡å…¥**: ã‚ãªãŸè‡ªèº«ã®çŸ¥è­˜ã‚„æ„è¦‹ã¯å®Œå…¨ã«æ’é™¤ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠã®æ€§æ ¼ã€ä¾¡å€¤è¦³ã€è©•ä¾¡ã‚¹ã‚¿ãƒ³ã‚¹ã«100%ãªã‚Šãã£ã¦è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
2.  **åŸºæº–ã®éµå®ˆ**: æŒ‡å®šã•ã‚ŒãŸè©•ä¾¡åŸºæº–ã®é …ç›®ã¨é…ç‚¹ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ãã‚Œä»¥å¤–ã®è¦³ç‚¹ã§è©•ä¾¡ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
3.  **JSONå½¢å¼ã§ã®å‡ºåŠ›**: çµæœã¯å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    -   ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚­ãƒ¼ã¯ `evaluations` ã¨ã—ã¦ãã ã•ã„ã€‚
    -   ãã®å€¤ã¯ã€å„å°æœ¬ã®ã€Œãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚’ã‚­ãƒ¼ã¨ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚
    -   å„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ `total_score` (æ•°å€¤), `feedback` (æ–‡å­—åˆ—), `breakdown` (ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ) ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
    -   `breakdown` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ã€è©•ä¾¡åŸºæº–ã®å„é …ç›®åã‚’ã‚­ãƒ¼ã¨ã—ã€`score` (æ•°å€¤) ã¨ `feedback` (æ–‡å­—åˆ—) ã‚’æŒã¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å€¤ã¨ã—ã¦ãã ã•ã„ã€‚

ã€å‡ºåŠ›å½¢å¼ã®ä¾‹ã€‘
{{
  "evaluations": {{
    "writer1_å°æœ¬1_loop1_... .md": {{
      "total_score": 85,
      "feedback": "ãƒšãƒ«ã‚½ãƒŠã«ãªã‚Šãã£ãŸç·åˆçš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯...",
      "breakdown": {{
        "ãƒ•ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ": {{
          "score": 22,
          "feedback": "æœ€åˆã®3ç§’ã®æ´ã¿ã¯ç´ æ™´ã‚‰ã—ã„..."
        }},
        "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ˜ç¢ºã•": {{
          "score": 18,
          "feedback": "ä¼ãˆãŸã„ã“ã¨ã¯åˆ†ã‹ã‚‹ãŒã€å°‘ã—è¡¨ç¾ãŒå†—é•·..."
        }}
      }}
    }},
    "writer1_å°æœ¬2_loop1_... .md": {{
      "total_score": 72,
      "feedback": "...",
      "breakdown": {{ ... }}
    }}
  }}
}}
"""
        # In a real implementation, you would use the 'anthropic' library
        # raw_json_response = client.messages.create(...)
        
        raw_json_response = self._call_anthropic_api(final_user_prompt)

        try:
            data = json.loads(raw_json_response)
            evaluations = data.get("evaluations", {})
            if isinstance(evaluations, dict) and evaluations:
                print(f"âœ… {self.persona_id}: å…¨å°æœ¬ã®è©•ä¾¡JSONã®è§£æã«æˆåŠŸã—ã¾ã—ãŸã€‚")
                return evaluations
            else:
                print(f"âŒ {self.persona_id}: LLMã‹ã‚‰ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™ã€‚")
                return {"error": "LLM response was not a valid evaluation dictionary.", "raw_response": raw_json_response}
        except json.JSONDecodeError:
            print(f"âŒ {self.persona_id}: LLMã‹ã‚‰ã®å¿œç­”ãŒJSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return {"error": "Failed to decode JSON from LLM response.", "raw_response": raw_json_response}

    def get_persona_profile(self) -> Dict[str, Any]:
        """
        Returns a generic description. The true profile is now defined by the prompt.
        """
        return {
            "personality": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãå‹•çš„äººæ ¼",
            "age_group": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã",
            "judgment_criteria": "MDã®æŒ‡ç¤ºã«ã‚ˆã‚Šæ¯å›å¤‰åŒ–"
        }