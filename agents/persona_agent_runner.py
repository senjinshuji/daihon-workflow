#!/usr/bin/env python3
"""
Persona Agent Runner - Automated Persona Evaluation Agents
Handles automated script evaluation based on dynamic, file-based instructions.
"""

import sys
import os
import re
import glob
import json
from datetime import datetime
from message_handler import AgentRunner
from persona_agents import PersonaAgent # Use the new flexible agent

class PersonaAgentRunner(AgentRunner):
    """Automated Persona Agent Runner"""
    
    def __init__(self, persona_id: str):
        super().__init__(persona_id)
        self.persona_id = persona_id
        # Create a single, flexible persona agent instance
        self.persona_agent = PersonaAgent(persona_id)
        
    def initialize_agent(self):
        """Initialize persona agent"""
        print(f"ğŸ“Š {self.persona_id.upper()} Agent è‡ªå‹•åŒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print("ğŸ“‹ CDè©•ä¾¡ä¾é ¼å¾…æ©Ÿä¸­...")
    
    def process_message(self, message: str):
        """Process incoming messages from CD"""
        print(f"ğŸ“¨ {self.persona_id} å—ä¿¡: {message[:100]}...")
        
        # New instruction format: [PROJECT:{project_name}] [LOOP:{loop_num}] [EVALUATE]
        match = re.search(r"\[PROJECT:(.+?)\] \[LOOP:(\d+)\] \[EVALUATE\]", message)
        
        if match:
            project_name = match.group(1)
            loop_num = int(match.group(2))
            self._handle_evaluation_task(project_name, loop_num)
        else:
            print(f"ğŸ’­ {self.persona_id}: æœªå¯¾å¿œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã§ã™ã€‚`[PROJECT:...] [LOOP:...] [EVALUATE]` å½¢å¼ã‚’å¾…æ©Ÿã—ã¾ã™ã€‚")
    
    def _handle_evaluation_task(self, project_name: str, loop_num: int):
        """Handles the entire evaluation process based on parsed instructions."""
        print(f"ğŸ“Š {self.persona_id} è©•ä¾¡ã‚¿ã‚¹ã‚¯é–‹å§‹... (Project: {project_name}, Loop: {loop_num})")

        project_path = os.path.join("projects", project_name)
        suffix = f"_loop{loop_num}" if loop_num > 1 else ""

        # 1. Read definition and criteria files
        try:
            persona_def_file = os.path.join(project_path, f"{self.persona_id}_definition{suffix}.md")
            criteria_file = os.path.join(project_path, f"persona_evaluation_criteria{suffix}.md")

            with open(persona_def_file, 'r', encoding='utf-8') as f:
                persona_def_content = f.read()
            with open(criteria_file, 'r', encoding='utf-8') as f:
                criteria_content = f.read()
        except FileNotFoundError as e:
            self._report_error(f"å¿…è¦ãªå®šç¾©/åŸºæº–ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
            return

        # 2. Find and read all script files for the current loop
        scripts_content, script_files_map = self._find_and_read_scripts(project_path, loop_num)
        if not scripts_content:
            self._report_error(f"Loop {loop_num} ã®è©•ä¾¡å¯¾è±¡å°æœ¬ãŒ '{project_path}' ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        # 3. Build the full prompt for the agent
        full_prompt = f"""
# ã‚ãªãŸã®ãƒšãƒ«ã‚½ãƒŠ (Your Persona)
{persona_def_content}

---

# è©•ä¾¡åŸºæº– (Evaluation Criteria)
{criteria_content}

---

# è©•ä¾¡å¯¾è±¡å°æœ¬ (Scripts to Evaluate)
{scripts_content}
"""
        # 4. Call the agent to evaluate all scripts at once
        evaluations = self.persona_agent.evaluate_scripts(full_prompt)
        
        if "error" in evaluations:
            self._report_error(f"å°æœ¬è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {evaluations['error']}", evaluations.get('raw_response'))
            return
            
        # 5. Format and send the completion report to CD
        report = self._create_evaluation_report(evaluations, script_files_map)
        self.send_message("cd", report)
        
        print(f"âœ… {self.persona_id} [Loop {loop_num}] ã®å…¨è©•ä¾¡ã‚¿ã‚¹ã‚¯å®Œäº†ã€‚æ¬¡ä¾é ¼å¾…æ©Ÿä¸­...")

    def _find_and_read_scripts(self, project_path: str, loop_num: int) -> tuple[str, dict]:
        """Finds all script files for the loop, reads them, and returns combined content."""
        print(f"  ğŸ” {project_path} å†…ã§ Loop {loop_num} ã®å°æœ¬ã‚’æ¤œç´¢ä¸­...")
        # Adhering to the new rule: scripts are in the project folder root
        # We also need to correct the writer to save here. For now, we search both.
        search_path_root = os.path.join(project_path, f"*_loop{loop_num}_*.md")
        search_path_results = os.path.join(project_path, "results", f"*_loop{loop_num}_*.md")
        
        script_files = glob.glob(search_path_root) + glob.glob(search_path_results)
        
        # Filter out non-script files like instructions
        script_files = [f for f in script_files if re.search(r'writer\d+_å°æœ¬\d+', os.path.basename(f))]

        if not script_files:
            return "", {}

        print(f"  ğŸ“‚ {len(script_files)}å€‹ã®å°æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã€‚")
        
        all_scripts_text = ""
        script_map = {}
        for i, file_path in enumerate(script_files):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_name = os.path.basename(file_path)
                script_map[f"script_{i+1}"] = file_name # Generic key for the mock, real would use filename
                
                all_scripts_text += f"## å°æœ¬ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name}\n\n"
                all_scripts_text += f"```markdown\n{content}\n```\n\n---\n\n"
            except Exception as e:
                print(f"   -> âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path} ({e})")
                continue
                
        return all_scripts_text, script_map

    def _create_evaluation_report(self, evaluations: dict, script_files_map: dict) -> str:
        """Creates a formatted evaluation report for the CD."""
        profile = self.persona_agent.get_persona_profile()
        report_title = f"ã€{self.persona_id.upper()} è©•ä¾¡å®Œäº†å ±å‘Šã€‘"
        
        report = f"""{report_title}

ãƒšãƒ«ã‚½ãƒŠã‚¿ã‚¤ãƒ—: {profile['personality']} ({self.persona_id})
è©•ä¾¡å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

"""
        # The mock response from the agent uses generic keys like "script_1".
        # A real response would use the filenames as keys. We handle both.
        # This mapping part is a bit of a hack to make the simulation work.
        is_mock_response = any(key.startswith("script_") for key in evaluations.keys())
        
        sorted_eval_items = sorted(evaluations.items(), key=lambda item: str(item[0]))

        for key, evaluation in sorted_eval_items:
            file_name = script_files_map.get(key, key) # Get real filename if available
            
            report += f"### è©•ä¾¡å¯¾è±¡: {file_name}\n"
            report += f"**ç·åˆã‚¹ã‚³ã‚¢: {evaluation.get('total_score', 'N/A')}/100ç‚¹**\n\n"
            report += f"**ç·è©•:**\n{evaluation.get('feedback', 'è©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚')}\n\n"
            
            if "breakdown" in evaluation and isinstance(evaluation['breakdown'], dict):
                report += "**è©³ç´°è©•ä¾¡:**\n"
                for criterion, details in evaluation['breakdown'].items():
                    score = details.get('score', 'N/A')
                    feedback = details.get('feedback', '')
                    report += f"- **{criterion}:** {score}ç‚¹\n  - {feedback}\n"
            report += "\n---\n"
            
        return report

    def _report_error(self, error_message: str, raw_response: str = None):
        """Reports an error to the console and to the CD."""
        print(f"âŒ {self.persona_id}: {error_message}")
        full_error = f"ã€{self.persona_id.upper()} ERRORã€‘\n{error_message}"
        if raw_response:
            full_error += f"\n\n[RAW_RESPONSE]\n{raw_response[:500]}..."
        self.send_message("cd", full_error)

def main():
    """Run Persona Agent"""
    if len(sys.argv) < 2:
        print("å®Ÿè¡Œå¼•æ•°ã‚¨ãƒ©ãƒ¼: python persona_agent_runner.py [persona1|persona2|persona3]")
        sys.exit(1)
        
    persona_id = sys.argv[1]
    
    runner = PersonaAgentRunner(persona_id)
    runner.run()

if __name__ == "__main__":
    main()