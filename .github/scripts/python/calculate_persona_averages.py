#!/usr/bin/env python3
"""
Calculate persona evaluation averages and create ranking
Phase 2 Step 2.5: ãƒšãƒ«ã‚½ãƒŠè©•ä¾¡å¹³å‡å€¤è¨ˆç®—ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
"""
import os
import sys
import argparse
import pandas as pd
import json
import logging
import statistics
from pathlib import Path
from datetime import datetime

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def setup_logging():
    """Setup logging configuration"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def load_persona_evaluation(product_name, persona_id):
    """å€‹åˆ¥ãƒšãƒ«ã‚½ãƒŠè©•ä¾¡çµæœã‚’èª­ã¿è¾¼ã¿"""
    logger = setup_logging()
    
    eval_file = Path(f"{product_name}/artifacts/{persona_id}_evaluation.json")
    
    if not eval_file.exists():
        logger.error(f"âŒ {persona_id}ã®è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {eval_file}")
        return None
    
    try:
        with open(eval_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"âœ… {persona_id}è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {data.get('evaluation_metadata', {}).get('total_scripts_evaluated', 0)}æœ¬")
        return data
    
    except Exception as e:
        logger.error(f"âŒ {persona_id}è©•ä¾¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def calculate_averages_and_ranking(product_name):
    """3ãƒšãƒ«ã‚½ãƒŠã®è©•ä¾¡å¹³å‡å€¤ã‚’è¨ˆç®—ã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ"""
    
    logger = setup_logging()
    
    # 3ã¤ã®ãƒšãƒ«ã‚½ãƒŠè©•ä¾¡ã‚’èª­ã¿è¾¼ã¿
    persona_evaluations = {}
    for persona_id in ['persona1', 'persona2', 'persona3']:
        eval_data = load_persona_evaluation(product_name, persona_id)
        if eval_data is None:
            logger.error(f"âŒ {persona_id}ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“")
            return False
        persona_evaluations[persona_id] = eval_data
    
    # å…¨å°æœ¬ã®IDã‚’å–å¾—
    all_script_ids = set()
    for persona_data in persona_evaluations.values():
        all_script_ids.update(persona_data['evaluations'].keys())
    
    logger.info(f"ğŸ“Š å¯¾è±¡å°æœ¬æ•°: {len(all_script_ids)}æœ¬")
    
    # å¹³å‡å€¤è¨ˆç®—
    averaged_results = {}
    detailed_analysis = {}
    
    for script_id in all_script_ids:
        scores = []
        detailed_scores = {}
        evaluation_reasons = []
        
        for persona_id, persona_data in persona_evaluations.items():
            if script_id in persona_data['evaluations']:
                eval_data = persona_data['evaluations'][script_id]
                total_score = eval_data['total_score']
                scores.append(total_score)
                
                # å‹•çš„ã«è©•ä¾¡è»¸ã‚’å–å¾—ï¼ˆcriteria.jsonã®å†…å®¹ã«åŸºã¥ãï¼‰
                axis_scores = {}
                for key, value in eval_data.items():
                    if key not in ['total_score', 'evaluation_reason'] and isinstance(value, (int, float)):
                        axis_scores[key] = value
                
                detailed_scores[persona_id] = {
                    'total_score': total_score,
                    **axis_scores  # å‹•çš„ãªè©•ä¾¡è»¸ã‚¹ã‚³ã‚¢ã‚’å±•é–‹
                }
                evaluation_reasons.append(f"{persona_id}: {eval_data.get('evaluation_reason', 'ç†ç”±ãªã—')}")
        
        if len(scores) == 3:
            # å¹³å‡å€¤è¨ˆç®—
            avg_score = statistics.mean(scores)
            score_variance = statistics.variance(scores) if len(scores) > 1 else 0
            
            # ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ãƒ¬ãƒ™ãƒ«åˆ¤å®š
            if score_variance <= 25:  # æ¨™æº–åå·®5ç‚¹ä»¥ä¸‹
                consensus_level = "high"
            elif score_variance <= 100:  # æ¨™æº–åå·®10ç‚¹ä»¥ä¸‹
                consensus_level = "medium"
            else:
                consensus_level = "low"
            
            averaged_results[script_id] = round(avg_score, 2)
            
            detailed_analysis[script_id] = {
                'persona_scores': scores,
                'average_score': round(avg_score, 2),
                'score_variance': round(score_variance, 2),
                'consensus_level': consensus_level,
                'detailed_scores': detailed_scores,
                'evaluation_reasons': evaluation_reasons,
                'divergent_reasons': f"åˆ†æ•£: {round(score_variance, 2)} ({'é«˜' if consensus_level == 'low' else 'ä¸­' if consensus_level == 'medium' else 'ä½'}ä¸€è‡´)"
            }
        else:
            logger.warning(f"âš ï¸ {script_id}: 3ãƒšãƒ«ã‚½ãƒŠå…¨ã¦ã®è©•ä¾¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ{len(scores)}å€‹ï¼‰")
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆï¼ˆå¹³å‡ã‚¹ã‚³ã‚¢é™é †ï¼‰
    ranking = sorted(averaged_results.items(), key=lambda x: x[1], reverse=True)
    
    # çµæœçµ±è¨ˆ
    all_avg_scores = list(averaged_results.values())
    result_stats = {
        'total_scripts': len(all_avg_scores),
        'average_of_averages': round(statistics.mean(all_avg_scores), 2) if all_avg_scores else 0,
        'median_score': round(statistics.median(all_avg_scores), 2) if all_avg_scores else 0,
        'score_range': {
            'min': round(min(all_avg_scores), 2) if all_avg_scores else 0,
            'max': round(max(all_avg_scores), 2) if all_avg_scores else 0
        },
        'high_consensus_count': len([a for a in detailed_analysis.values() if a['consensus_level'] == 'high']),
        'medium_consensus_count': len([a for a in detailed_analysis.values() if a['consensus_level'] == 'medium']),
        'low_consensus_count': len([a for a in detailed_analysis.values() if a['consensus_level'] == 'low'])
    }
    
    # çµæœä¿å­˜
    final_results = {
        'calculation_metadata': {
            'timestamp': datetime.now().isoformat(),
            'product_name': product_name,
            'calculation_method': '3ãƒšãƒ«ã‚½ãƒŠå¹³å‡å€¤',
            'personas_included': ['persona1', 'persona2', 'persona3'],
            'total_scripts_processed': len(averaged_results)
        },
        'averaged_scores': averaged_results,
        'ranking': [{'rank': i+1, 'script_id': script_id, 'average_score': score} 
                   for i, (script_id, score) in enumerate(ranking)],
        'detailed_analysis': detailed_analysis,
        'result_statistics': result_stats,
        'persona_summaries': {
            persona_id: {
                'persona_summary': data.get('persona_summary', f'{persona_id}è¦ç´„'),
                'total_scripts_evaluated': data.get('evaluation_metadata', {}).get('total_scripts_evaluated', 0),
                'average_score_given': round(statistics.mean([
                    eval_data['total_score'] 
                    for eval_data in data['evaluations'].values()
                ]), 2) if data['evaluations'] else 0
            }
            for persona_id, data in persona_evaluations.items()
        }
    }
    
    # ä¿å­˜
    artifacts_dir = Path(f"{product_name}/artifacts")
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = artifacts_dir / 'persona_evaluation_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… ãƒšãƒ«ã‚½ãƒŠè©•ä¾¡å¹³å‡å€¤è¨ˆç®—å®Œäº†")
    logger.info(f"ğŸ“Š å‡¦ç†å°æœ¬æ•°: {len(averaged_results)}æœ¬")
    logger.info(f"ğŸ“ˆ å¹³å‡ã‚¹ã‚³ã‚¢ç¯„å›²: {result_stats['score_range']['min']} - {result_stats['score_range']['max']}")
    logger.info(f"ğŸ¯ é«˜ä¸€è‡´: {result_stats['high_consensus_count']}æœ¬, ä¸­ä¸€è‡´: {result_stats['medium_consensus_count']}æœ¬, ä½ä¸€è‡´: {result_stats['low_consensus_count']}æœ¬")
    logger.info(f"ğŸ“ ä¿å­˜å…ˆ: {output_file}")
    
    # TOP5ã‚’è¡¨ç¤º
    logger.info("ğŸ† TOP5ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    for i, item in enumerate(ranking[:5]):
        script_id, score = item
        logger.info(f"  {i+1}ä½: {script_id} ({score}ç‚¹)")
    
    return len(averaged_results)

def main():
    parser = argparse.ArgumentParser(description='Calculate persona evaluation averages and ranking')
    parser.add_argument('--product-name', required=True, help='Product name')
    
    args = parser.parse_args()
    
    try:
        scripts_count = calculate_averages_and_ranking(args.product_name)
        print(f"scripts_processed={scripts_count}")
        print(f"calculation_completed={'true' if scripts_count > 0 else 'false'}")
        
    except Exception as e:
        logger = setup_logging()
        logger.error(f"âŒ å¹³å‡å€¤è¨ˆç®—ã«å¤±æ•—: {str(e)}")
        print("scripts_processed=0")
        print("calculation_completed=false")
        sys.exit(1)

if __name__ == "__main__":
    main()
