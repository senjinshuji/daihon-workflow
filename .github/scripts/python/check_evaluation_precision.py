#!/usr/bin/env python3
"""
Check evaluation precision by comparing persona rankings with original groups
Phase 2 Step 3: ç²¾åº¦ç¢ºèª
"""
import os
import sys
import argparse
import json
import pandas as pd
import logging
from pathlib import Path

def setup_logging():
    """Setup logging configuration"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def check_precision(product_name):
    """ãƒšãƒ«ã‚½ãƒŠè©•ä¾¡ã®ç²¾åº¦ç¢ºèª"""
    
    logger = setup_logging()
    artifacts_dir = Path(f"{product_name}/artifacts")
    
    # å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    sample_file = artifacts_dir / 'sample_scripts_for_evaluation.json'
    evaluation_file = artifacts_dir / 'persona_evaluation_results.json'
    
    if not sample_file.exists() or not evaluation_file.exists():
        logger.error("âŒ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False, 0, True
    
    with open(sample_file, 'r', encoding='utf-8') as f:
        sample_data = json.load(f)
    
    with open(evaluation_file, 'r', encoding='utf-8') as f:
        evaluation_data = json.load(f)
    
    # å…ƒã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒãƒƒãƒ”ãƒ³ã‚°
    original_groups = sample_data['group_mapping']
    
    # ãƒšãƒ«ã‚½ãƒŠè©•ä¾¡ã«åŸºã¥ããƒ©ãƒ³ã‚­ãƒ³ã‚°
    script_scores = evaluation_data.get('averaged_scores', {})
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    sorted_scripts = sorted(script_scores.items(), key=lambda x: x[1], reverse=True)
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«åŸºã¥ãã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
    total_scripts = len(sorted_scripts)
    group_size = total_scripts // 3
    
    evaluated_groups = {}
    
    # ä¸Šä½ã‚°ãƒ«ãƒ¼ãƒ—
    for i in range(group_size):
        script_id = sorted_scripts[i][0]
        evaluated_groups[script_id] = 'top'
    
    # ä¸­ä½ã‚°ãƒ«ãƒ¼ãƒ—
    for i in range(group_size, group_size * 2):
        script_id = sorted_scripts[i][0]
        evaluated_groups[script_id] = 'middle'
    
    # ä¸‹ä½ã‚°ãƒ«ãƒ¼ãƒ—
    for i in range(group_size * 2, total_scripts):
        script_id = sorted_scripts[i][0]
        evaluated_groups[script_id] = 'bottom'
    
    # ç²¾åº¦è¨ˆç®—ï¼šå„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ä¸€è‡´ç‡
    group_accuracy = {}
    
    for group in ['top', 'middle', 'bottom']:
        original_scripts = [sid for sid, grp in original_groups.items() if grp == group]
        evaluated_scripts = [sid for sid, grp in evaluated_groups.items() if grp == group]
        
        # ä¸€è‡´ã™ã‚‹å°æœ¬æ•°
        correct_scripts = set(original_scripts) & set(evaluated_scripts)
        
        group_accuracy[group] = {
            'original_count': len(original_scripts),
            'evaluated_count': len(evaluated_scripts),
            'correct_count': len(correct_scripts),
            'accuracy': len(correct_scripts) / len(original_scripts) if original_scripts else 0
        }
    
    # å…¨ä½“ç²¾åº¦ã‚’ãƒˆãƒƒãƒ—ã‚°ãƒ«ãƒ¼ãƒ—ã®ç²¾åº¦ã®ã¿ã§è¨ˆç®—
    overall_accuracy = group_accuracy['top']['accuracy']
    
    # ãƒˆãƒƒãƒ—ã‚°ãƒ«ãƒ¼ãƒ—ã®ç²¾åº¦ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã«æœ€é©åŒ–ãŒå¿…è¦
    needs_optimization = False
    top_correct = group_accuracy['top']['correct_count']
    top_total = group_accuracy['top']['original_count']
    
    # ãƒˆãƒƒãƒ—ã‚°ãƒ«ãƒ¼ãƒ—ã§5æœ¬ä¸­4æœ¬ä»¥ä¸Šæ­£è§£ã—ã¦ã„ãªã„å ´åˆã¯æœ€é©åŒ–ãŒå¿…è¦
    if top_correct < 4:
        needs_optimization = True
        logger.warning(f"âš ï¸ ãƒˆãƒƒãƒ—ã‚°ãƒ«ãƒ¼ãƒ—ã®ç²¾åº¦ãŒä¸è¶³: {top_correct}/{top_total}æœ¬ã®ã¿æ­£è§£")
    
    # çµæœä¿å­˜
    precision_result = {
        'overall_accuracy': overall_accuracy,
        'overall_accuracy_percent': round(overall_accuracy * 100, 2),
        'group_accuracy': group_accuracy,
        'original_groups': original_groups,
        'evaluated_groups': evaluated_groups,
        'needs_optimization': needs_optimization,
        'ranking_details': {
            'sorted_scripts': sorted_scripts,
            'score_distribution': script_scores
        },
        'precision_threshold': 80.0,
        'meets_threshold': overall_accuracy >= 0.8,  # ãƒˆãƒƒãƒ—ã‚°ãƒ«ãƒ¼ãƒ—ã®ç²¾åº¦ãŒ80%ä»¥ä¸Š
        'check_timestamp': pd.Timestamp.now().isoformat()
    }
    
    output_file = artifacts_dir / 'precision_check.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(precision_result, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š ãƒˆãƒƒãƒ—ã‚°ãƒ«ãƒ¼ãƒ—ç²¾åº¦: {precision_result['overall_accuracy_percent']}% ({top_correct}/{top_total}æœ¬)")
    logger.info(f"ğŸ¯ æœ€é©åŒ–å¿…è¦: {'ã¯ã„' if needs_optimization else 'ã„ã„ãˆ'}")
    logger.info(f"âœ… é–¾å€¤é”æˆ: {'ã¯ã„ (5æœ¬ä¸­4æœ¬ä»¥ä¸Šæ­£è§£)' if precision_result['meets_threshold'] else 'ã„ã„ãˆ'}")
    
    return precision_result['meets_threshold'], precision_result['overall_accuracy_percent'], needs_optimization

def main():
    parser = argparse.ArgumentParser(description='Check evaluation precision')
    parser.add_argument('--product-name', required=True, help='Product name')
    
    args = parser.parse_args()
    
    try:
        meets_threshold, accuracy, needs_opt = check_precision(args.product_name)
        
        print(f"precision_check={'pass' if meets_threshold else 'fail'}")
        print(f"accuracy_rate={accuracy}")
        print(f"needs_optimization={'true' if needs_opt else 'false'}")
        
    except Exception as e:
        logger = setup_logging()
        logger.error(f"âŒ ç²¾åº¦ç¢ºèªã«å¤±æ•—: {str(e)}")
        print("precision_check=fail")
        print("accuracy_rate=0")
        print("needs_optimization=true")
        sys.exit(1)

if __name__ == "__main__":
    main()
